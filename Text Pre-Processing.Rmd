---
title: "Text Pre-Processing Code"
output: html_document
---

```{r, include=FALSE}

#### GLOBAL OPTIONS (REPORT RENDERING)

# include=FALSE means that this particular R code chunk will not be included in the final report. Useful for global settings or bits of code that might not be necessary to show in the final report (settings/set-up lines of code).
if (!("knitr" %in% installed.packages())) {
  install.packages('knitr', repos='http://cran.rstudio.org')}
library(knitr)

if (!("formatR" %in% installed.packages())) {
  install.packages("formatR")}
library(formatR)

knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center",
                      tidy.opts=list(width.cutoff=70), tidy=TRUE) #to avoid source code going out of bounds
                                                                  #does not work if single var name is too
                                                                  #long though, as in point 5 below!  

#this line is used to specify any global settings to be applied to the R Markdown script.   The example sets all code chunks as “echo=TRUE”, meaning they will be included in the final rendered version, whereas error/warning messages or any other messages from R will not be displayed in the final, 'knitted' R Markdown file.

### permanently setting the CRAN mirror (avoids error "trying to use CRAN without setting a mirror")

local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org"
       options(repos=r)})

```




```{r, include = F}

#### INSTALL RELEVANT PACKAGES

if (!("quanteda" %in% installed.packages())) {
  install.packages("quanteda")
}

if (!("quanteda.textmodels" %in% installed.packages())) {
  install.packages("quanteda.textmodels")
}

if (!("quanteda.textplots" %in% installed.packages())) {
  install.packages("quanteda.textplots")
}

if (!("quanteda.textstats" %in% installed.packages())) {
  install.packages("quanteda.textstats")
}

if (!("devtools" %in% installed.packages())) {
  install.packages("devtools")
}

if (!("quanteda.corpora" %in% installed.packages())) {
  devtools::install_github("quanteda/quanteda.corpora")
}

if (!("readtext" %in% installed.packages())) {
  install.packages("readtext")
}

if (!("dplyr" %in% installed.packages())) {
  install.packages("dplyr")
}

if (!("tidytext" %in% installed.packages())) {
  install.packages("tidytext")
}

if (!("ggplot2" %in% installed.packages())) {
  install.packages("ggplot2")
}

if (!("rtweet" %in% installed.packages())) {
  install.packages("rtweet")
}

require(quanteda)
require(quanteda.textmodels)
require(quanteda.textplots)
require(quanteda.textstats)
require(quanteda.corpora)
require(readtext)
require(dplyr)
require(tidytext)
require (ggplot2)
require(rtweet)

```


```{r, include=F}
##### REMEMBER: WORKING DIRECTORY!

# Either Session --> Set Working Directory --> To Source File Location (should do it automatically, but only when knitting - so, if you want to run the code bit by bit first do set the wd here). To Source File Location means that the data will be imported and saved IN THE SAME FOLDER where you saved the RMarkdown file you're working with!

## OR

## setwd("")

```


## Task 1: Reading Text Data into R & Transforming to a Corpus Object ##

"Corpus" is a term from linguistics and it indicates a collection of documents. A Corpus class object in R organises your text data and all the relevant meta-data allowing subsequent easy processing of texts and text-descriptive variables (meta-data - e.g. author, dates, etc ...). A necessary first step in text mining is therefore to transform the datafiles containing your texts and meta-data (usually stored in a .csv) into a corpus object, as done below:

```{r}
## Load Speeches
speeches <-readtext("scraped_speech_data.csv", text_field = "text", encoding = "ISO-8859-1") %>%
  select(-URL)

#generate author variable
speeches <- speeches %>%  
   mutate(author = case_when(grepl("Mordaunt", speakers) ~ "Penny Mordaunt",
                             grepl("Newlands", speakers) ~ "Gavin Newlands")) %>%
  select(-speakers, -ID)

```

Note that instead of using read.csv we have to use the readtext function and specify what is the text column in our .csv file. This will make sure that R will distinguish the text column from the other columns in the file containing meta-data information.

```{r}
#convert to corpus object
SpeechCorpus<-corpus(speeches)
summary(SpeechCorpus)
ndoc(SpeechCorpus)

#subset by author
SpeechCorpus_Mordaunt <- corpus_subset(SpeechCorpus, author == "Penny Mordaunt")
SpeechCorpus_Newlands <- corpus_subset(SpeechCorpus, author == "Gavin Newlands")


```


## Task 2: Initial Descriptives ##

To retrieve useful information summarising total number of words used (tokens) or total number of unique words used (types) or total sentences you should first save the summary(corpus) as an object and extract its relevant variables. 

```{r}
#total number of documents in each corpus:
n_MP1<-ndoc(SpeechCorpus_Mordaunt)
n_MP1
n_MP2<-ndoc(SpeechCorpus_Newlands)
n_MP2

# types, tokens, sentence summaries
sumSpeechCorpus1<-summary(SpeechCorpus_Mordaunt, n=n_MP1)
summary(sumSpeechCorpus1$Tokens)
summary(sumSpeechCorpus1$Types)
summary(sumSpeechCorpus1$Sentences)

sumSpeechCorpus2<-summary(SpeechCorpus_Newlands, n=n_MP2)
summary(sumSpeechCorpus2$Tokens)
summary(sumSpeechCorpus2$Types)
summary(sumSpeechCorpus2$Sentences)

#If you want, you can also save this summary outputs in a csv file:
write.csv(sumSpeechCorpus1, file="SpeechCorpus_Mordaunt_Summary.csv", row.names=FALSE)
write.csv(sumSpeechCorpus2, file="SpeechCorpus_Newlands_Summary.csv", row.names=FALSE)

```

## Task 3: Keywords in Context ##

Another important step of the descriptive analysis of text data is to explore how particular words/concepts that you might be interested in are used. KWICs - Keywords in Context - show the word of interest (keyword) embedded in a number of precedent and subsequent words. The lines of code below show how to visualise KWICs and how to specify a word 'window'.

```{r}
#exploring KWICs

kwic(tokens(SpeechCorpus_Mordaunt), pattern = "sovereign*", window=20)
kwic(tokens(SpeechCorpus_Newlands), pattern = "sovereign*", window=20)


#multi-word KWICs
kwic(tokens(SpeechCorpus_Mordaunt), pattern = phrase("United Kingdom"), window=3)

```


## Task 4: Document-Feature Matrices & Text Data Cleaning ##

The document-feature matrix is the building block of any text-mining exercise. The code below converts a corpus object into a dfm object. Moreover, the code below allows to 'pre-process' the text data in the corpus. Dfm object options such as "remove=" or "stem=" allow to, for example, eliminate stop-words, punctuation and to stem words from texts.

```{r}

DFM_1 <- tokens(SpeechCorpus_Mordaunt, 
                   remove_punct = TRUE, 
                   remove_symbols = T, 
                   remove_numbers=T, 
                   remove_url=T) %>%
                   dfm() %>%
                   dfm_tolower()


#check top words:
topfeatures(DFM_1, n = 20)


DFM_2<- tokens(SpeechCorpus_Newlands, 
                   remove_punct = TRUE, 
                   remove_symbols = T, 
                   remove_numbers=T, 
                   remove_url=T) %>%
                   dfm() %>%
                   dfm_tolower()

#check top words:
topfeatures(DFM_2, n = 20)


#It looks like stopwords are taking centre-stage. Let's explore stopword lists & then eliminate them from our dfm, as unlikely to be useful in text analysis (may only capture rhetorical style).

#exploring stopwords
head(stopwords("en"), 100)
head(stopwords("it"), 100)
head(stopwords("fr"), 100)

#to remove stopwords:
DFM_1<- dfm_remove(DFM_1, pattern = stopwords("en"))
#check top words:
topfeatures(DFM_1, n = 20)


DFM_2<- dfm_remove(DFM_2, pattern = stopwords("en"))
#check top words:
topfeatures(DFM_2, n = 20)

# to remove other words you think irrelevant (stopword lists may not be comprehensive)

DFM_1<- dfm_remove(DFM_1, pattern = c("one", "also", "thank", "many", "hon", "can", "member"))
DFM_2 <- dfm_remove(DFM_2, pattern = c("one", "also", "thank", "many", "hon", "can", "member"))


```

## Task 5: Stemming ##

Stemming is a step that brings all tokens to their root form (see: https://quanteda.io/reference/tokens_wordstem.html). This helps in reducing the sparsity of the dfm and improve computational efficiency. You need to think carefully about this as stemming may hide meaningful variation (e.g. police and policy will become the same type, as their stem is the same).

```{r}
DFM_1_stem <- dfm_wordstem(DFM_1)
DFM_2_stem <- dfm_wordstem(DFM_2)
```


## Task 6: Term Frequency, Inverse Document Frequency Weighting ##

Sometimes you might wish to 'weight' words according to their frequency within documents and across the corpus. This is achieved by the tf-idf measure. The dfm object can accommodate - via the "scheme" option - this additional transformation. See the code below for the implementation of tf-idf weighting in a dfm. This is not recommended if the aim of your text analysis involves scaling (e.g. Benoit 2020), but can help the computational efficiency of text classifiers.

```{r}
#tf-idf

DFM_1_tfidf<- DFM_1 %>%
  dfm_tfidf(scheme_tf = "count", scheme_df = "inverse",base = 10) 


DFM_2_tfidf<- DFM_2 %>%
  dfm_tfidf( scheme_tf = "count", scheme_df = "inverse",base = 10) 

```


## Task 7: Text Descriptives: WORDCLOUDS ## 

We introduced wordclouds last week: a wordcloud is a visualisation of the most common words in a corpus of texts. The function **textplot_wordcloud** from quanteda is helpful in building wordclouds. The function has several options that allows to customise the look of the wordcloud. For more options see: https://quanteda.io/reference/textplot_wordcloud.html

```{r}
# wordcloud

textplot_wordcloud(DFM_1_stem, 
                   min_count = 5,
                   max_words = 150,
                   min_size = 0.25,
                   max_size = 3,
                   random_order = FALSE, 
                   rotation = 0.25,
                   color = "darkblue")


textplot_wordcloud(DFM_2_stem, 
                   min_count = 5,
                   max_words = 150,
                   min_size = 0.25,
                   max_size = 3,
                   random_order = FALSE, 
                   rotation = 0.25,
                   color = "sienna")



```



**Task 8: Group Analysis**

It is possible to collate the two different corpora and then run a comparative analysis by splitting the dfm by author. This is what the code below does.

```{r}

#Do all the relevant cleaning steps above using the "Master Corpus":

DFM_Master<- tokens(SpeechCorpus, 
                   remove_punct = TRUE, 
                   remove_symbols = T, 
                   remove_numbers=T, 
                   remove_url=T) %>%
                   dfm() %>%
                   dfm_tolower()


#check top words:
topfeatures(DFM_Master, n = 20)


# remove stopwords:
DFM_Master<- dfm_remove(DFM_Master, pattern = stopwords("en"))
#check top words:
topfeatures(DFM_Master, n = 20)


# to remove other words you think irrelevant (stopword lists may not be comprehensive)

DFM_Master <- dfm_remove(DFM_Master, pattern = c("one", "also", "thank", "many", "hon", "can", "member"))


# To eyeball word frequencies by author (i.e. keyness):

prop_dfm_byauthor<- tokens(SpeechCorpus,
                           remove_punct = TRUE,
                           remove_symbols = T, 
                           remove_numbers=T,    
                           remove_url=T) %>%
                           dfm() %>%
                           dfm_tolower() %>%
                           dfm_remove(pattern = stopwords("en")) %>%
                           dfm_remove(pattern = c("one", "also", "thank", "many", "hon", "can", "member")) %>%
                          dfm_group(groups = author) %>%
                          dfm_weight(scheme = "prop")


write.csv(convert(prop_dfm_byauthor, to = "data.frame"), file="dfm_group.csv", row.names=FALSE)
    

```

## Task 9: Analysing Author Differences in a Single Plot ##

To inspect and plot differences in word usage between two authors, see the lines of code below:

```{r}

#we can sort and inspect groups

DFM_Master_Group<-dfm_sort(DFM_Master)

#use words up to 20th rank

DFM_Master_Group<-textstat_frequency(
  DFM_Master_Group,
  n = 20,
  groups = author)

#plot for comparison (using dfm with top 20th ranked words only)

ggplot(data = DFM_Master_Group, aes(x = factor(nrow(DFM_Master_Group):1), y = frequency)) +
    geom_point() +
    facet_wrap(~ group, scales = "free") +
    coord_flip() +
    scale_x_discrete(breaks = nrow(DFM_Master_Group):1,
                       labels = DFM_Master_Group$feature) +
    labs(x = NULL, y = "Relative frequency")


```


## Lab Exercise ##

Play around with the code above and with the quanteda tutorials mentioned above and in the lecture slides, using your scraped speeches. This will form part of your assignment, so good to get started now!

1. Create your Assignment 2 .Rmd file

2. Reproduce code above using your own speech data!
